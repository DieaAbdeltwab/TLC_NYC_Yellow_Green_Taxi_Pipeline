# ğŸš• NYC TLC Yellow & Green Taxi Data Pipeline

An end-to-end **Data Engineering** pipeline built with a modern stack using **MinIO**, **Apache Spark**, and dimensional modeling â€” processing over **48 million** raw rows of NYC taxi trip data (Yellow & Green, year 2024 only) and transforming it into clean, structured, and analytics-ready datasets.

---

## ğŸ“Œ Project Summary

| Stage        | Description                                                 |
| ------------ | ----------------------------------------------------------- |
| ğŸ”¸ Ingestion | 2024 NYC TLC Yellow & Green trip data loaded into **MinIO** |
| ğŸ”¸ ETL       | Spark job to clean, deduplicate, and transform raw data     |
| ğŸ”¸ Modeling  | Star schema built in **Spark** and exported to PostgreSQL   |
| ğŸ”¸ Output    | \~41 million cleaned, modeled rows stored in **PostgreSQL** |

---

## ğŸ› ï¸ Stack

- âš™ï¸ **MinIO** â€“ Data lake / object storage
- âš¡ **Apache Spark** (PySpark) â€“ Processing & modeling
- ğŸ˜ **PostgreSQL** â€“ Final star-schema warehouse
- ğŸ“Š **Power BI** â€“ Dashboarding and reporting
- ğŸ—‚ï¸ **Parquet** â€“ Raw data format (columnar)
- ğŸ³ **Docker Compose** â€“ Containerized environment

---

## ğŸ“¥ 1. Data Ingestion

Raw data for **2024** (Yellow & Green taxi trips) is downloaded from [NYC TLC Trip Records](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and uploaded to MinIO.

```bash
# Example: Download and upload to MinIO
wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet
mc alias set localminio http://localhost:9000 minioadmin minioadmin
mc mb localminio/tlc-data
mc cp yellow_tripdata_2024-01.parquet localminio/tlc-data/
```

---

## âš™ï¸ 2. ETL & Data Cleaning

Using **PySpark**, we:

- âœ… Removed invalid GPS coordinates
- âœ… Dropped null & duplicate records
- âœ… Filtered out trips with:
  - Zero/negative distance or fare
  - Pickup/dropoff year outside range
  - Invalid rate codes / payment types

**ğŸ“Š Before:** `48M rows`\
**ğŸ§¼ After:** `41M rows`

---

## ğŸ§± 3. Data Modeling (Star Schema)

Data modeling was performed **inside Spark**, producing a well-structured star schema exported to PostgreSQL.



- `dim_time` â€“ Time breakdown (hour, day, month, weekday)
- `dim_vendor` â€“ Taxi vendors (1=Creative, 2=Verifone)
- `dim_payment_type` â€“ Payment methods
- `dim_rate_code` â€“ NYC taxi rate codes
- `dim_pickup_location` â€“ Zones & coordinates
- `dim_dropoff_location` â€“ Zones & coordinates
- `dim_trip_category` â€“ Derived trip types (e.g. short, long)

---

## ğŸ“ˆ 4. Output & Usage

The final modeled tables are saved to:

- **PostgreSQL** for structured storage
- **Power BI** for dashboarding and data visualization

Power BI connects directly to PostgreSQL to create:

- ğŸ“Š Trip distribution analysis
- ğŸ’° Revenue and fare breakdowns
- ğŸ“ Popular pickup/dropoff zones
- â±ï¸ Time-based patterns and trends

---



